sudo docker run --gpus all -it \
    -v /home/galinyshka/models/llama:/models \
    -v /home/galinyshka/code/articles_project/zeroshot/schema.json:/schema.json \
    -p 8000:8000 \
    ghcr.io/ggml-org/llama.cpp:server-cuda \
    -m /models/llama-7b.Q4_K_M.gguf \
    --port 8000 \
    --host 0.0.0.0 \
    -n 1024 \
    --cache-ram 0


docker run --runtime nvidia --gpus all \
    --name vllm_qwen25_7b_instinct\
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HUGGING_FACE_HUB_TOKEN=" \
    -p 8000:8000 \
    vllm/vllm-openai:latest \
    Qwen/Qwen2.5-7B-Instruct-AWQ

# Load and run the model:
docker exec -it vllm_qwen25_7b_instinct bash -c "vllm serve Qwen/Qwen2.5-7B-Instruct-AWQ"

docker stop vllm_qwen25_7b_instinct
docker rm vllm_qwen25_7b_instinct
docker run --runtime nvidia --gpus all \
    --name vllm_qwen25_7b_instinct \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HUGGING_FACE_HUB_TOKEN=" \
    -p 8000:8000 \
    vllm/vllm-openai:latest \
    Qwen/Qwen2.5-7B-Instruct-AWQ \
    --max-num-seqs 32 \
    --gpu-memory-utilization 0.85


# Call the server using curl:
curl -X POST "http://localhost:8000/v1/chat/completions" \
	-H "Content-Type: application/json" \
	--data '{
		"model": "Qwen/Qwen2.5-7B-Instruct-AWQ",
		"messages": [
			{
				"role": "user",
				"content": "What is the capital of France?"
			}
		]
	}'
